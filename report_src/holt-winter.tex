\subsubsection{Presentation}
An intuitive assumption about the way one can predict the future is that the future should resemble the past experiences. So the most naive approach in forecasting is just to set the position of a future datapoint to the same place as the last observed one. One of common approaches used for forecasting is called \textit{exponential smoothing}. In a simple version we just copy and paste historical data. This however disregards historical data, except for the most recent ones. A more thoughtful approach would be to assign some weight to each data. The more recent a datapoint will be, the bigger its weight value. This is the main idea behind exponential smoothing.\\
But the simple exponential smoothing can not account for forecasting time series data that has a trend and/or a seasonal component. This issue can be resolved by using an upgraded version of smoothing called Holt-Winters method \cite{holt}.
It divides a time series $Y_{t}$ in three parts: one related to its seasonality ($s_{t}$), one related to its  trend behavior ($b_{t}$) and one for  the residual part ($l_{t}$). For each  of them, a simple EWMA is  applied  to predict a new value.  A  combination  of  these  expressions  is  used  to  estimate  $Y_{t+1}$. The following equations represent the HW computation: \\
\begin{align*}\\
  \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
  s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m}
\end{align*}
\subsubsection{Use-case}
Going back to the use case of this project, the implementation of this algorithm would theoretically  be able to generate a prediction, as well as extract a series of interesting knowledge about historical data. \\
First of all it would be able to find an existing \textit{trend} in the data. This trend will probably depend on the rate of new cars joining  the ConnectedDrive network over time, or on the growing popularity of certain services (that would increase the number of daily request from a car) and it will be best visible after a large amount of data is collected. \\
Secondly, it should pick up on any \textit{seasonal components}, if they exist, and give a hint towards existing outside factors that can influence the traffic. A hypothesis can be the fact that in summer, when kids do not have school and lots of people chose to go on vacations, the number of cars on the streets is smaller compared to winter. Once again, having enough data (for at least a year), these type of hypothesis can be tested by training the model and decomposing the data.\\

Currently, the amount of real data that was put at disposal for this project was enough to train a model, but not exactly enough to identify hidden knowledge. The data was simply too little to pick up any significant information, except for the evident ones like the fact that the data has a weekly and a daily pattern. \\

To see if such an algorithm is applicable to a dataset, first of all we can run a decomposition function that would try to split a batch of data into 3. In figure \ref{fig:holt_winter_decomposition} (Part.1) you can see the results from decomposing the BMW dataset. If we analyse the graphs, we can observe that the model is doing a fairly good job at handling the data. We purposely included the anomalous first two days (21-22 December) into our training data to observe how well does the model pick up on the existing trend as well as how easy it is to through it off. We can observe that the trends are well preserved, although their specifics are dimmed down.\\
\begin{figure}[h]
    \includegraphics[width=1\textwidth]{images/holt-winter-decomposition.png}
    \caption{Data decomposition into Trend, Seasonality and Residual using Holt-Winter Method }
    \label{fig:holt_winter_decomposition}
\end{figure}

In comparison, the seasonal graph does not do so well, while it does detect a daily pattern, it only plots almost gaussian distribution of daily data. This is due to the poor quality of data, that includes a holiday period, where the rush hour pattern is non-existent. Another factor that diminishes the pattern is the fact that the model averages over weekends, where again the double-peaked pattern disappears. If we exclude this holiday period, we can see a significant improvement in the pattern (see fig. \ref{fig:holt_winter_decomposition} Part 2).\\

Now that we know that our data is suitable for a Holt-Winter's Method analysis, we run divided the data into training and test data.
\begin{lstlisting}[language=Python, caption=Exponential Smoothing method]
    model = ExponentialSmoothing(train, seasonal_periods=seasonl, seasonal='mul').fit()
    pred = model.predict(start=test.index[0], end=test.index[-1])
\end{lstlisting}
The \textit{statsmodel} module in Python has a pre-built function to do the so-called \textbf{Triple Exponential Smoothing}. In this method, we can implement both the \textit{additive} and \textit{multiplicative} technique. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series.
The constants \textbf{$\alpha, \beta, \gamma$} (that were mentioned in the formulas above) - can be estimated (usually through a trial and error process known as fitting) and can be left for the algorithm to estimate or manually set in the .fit()\footnote{\url{https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.fit.html#statsmodels.tsa.holtwinters.ExponentialSmoothing.fit}} method as following:
\begin{lstlisting}
ExponentialSmoothing.fit(
    smoothing_level=None, smoothing_slope=None, 
    smoothing_seasonal=None, damping_slope=None, 
    optimized=True, use_boxcox=False, 
    remove_bias=False, use_basinhopping=False, 
    start_params=None, initial_level=None, 
    initial_slope=None, use_brute=True
)
\end{lstlisting}
\subsubsection{Results}
\begin{figure}
    \includegraphics[width=1\textwidth]{images/holt-winter-result.png}
    \caption{Results of Holt-Winter's prediction model}
    \label{fig:holt_winter_results}
\end{figure}
Bellow you can see the results of running such an algorithm. Although the \textbf{RSME} is about \textbf{48K}, which is a relatively good performance and as seen in  fig. \ref{fig:holt_winter_results} the prediction results are close to the reality, they still have the problem of recreating the double peaks pattern as it has no means of separating the weekdays from weekends, and end up averaging everything together. In addition, because the model puts together three distributions (trend, seasonality and residual) every time it trains, such an algorithm requires a lot of computational power.\\
This being said, with a reasonable amount of data for training, for at least a full year, this model should perform significantly better, being able to detect deeper knowledge, beyond the known weekly and daily pattern.