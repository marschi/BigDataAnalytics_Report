Taking in consideration all the approaches mentioned throughout the paper for both prediction and anomaly detection models, and the fact that for BMW this project is in an early development stage, meaning that it does not possess a great amount of data to train and validate models, here are the project recommendations:
\begin{enumerate}
    \item Start the framework by implementing a \textit{Mean Predictor} algorithm for both anomaly detection and prediction. It trains well for small amount of data, takes less computational power, and will offer reliable results.
    \item Gather data for at least a year and save all the data in different granularity buckets. Plot and test the data and see what granularity offers best results compared to the computational power needed to process it. In terms of this project, 5 minutes granularity buckets proved to perform good, preserving all the anomalies while requiring less computational power to pre-process. But the case might be different once more data is acquired.
    \item Add a \textit{geolocation} component to the data and create different data pools for each regions of the globe, instead of smashing all information into one bucket. Such a change might give new insights into the gathered information. For example, it might reveal different holiday patterns for different countries, as well as take into consideration the specifics of holidays for different regions (christian celebration vs. Muslim celebrations vs. Asian holidays).  
    \item After having enough data, eventually even data that includes a location component too, run it through a \textit{DeepAR} model. As this algorithm allows the analysis of a couple of sources of data at the same time, try including other sources of data like weather or gas prices. This algorithm (for a significant amount of data) can find some dependencies between different sources of data and significantly improve the predictions.
    \item In a year or two, implement a \textit{Random Cut Forest} model. After gathering some expert knowledge in handling anomalies, as well as information about the cause behind the anomalies and which deviations are worth looking into, you will have a better intuition about the right threshold. It might be an interesting idea to set a couple of threshold for different levels of anomalies, if this actually proves to bring any advantage in the way the anomalies are managed down the line.
    \item Run the data (after at least a year) through a \textit{Holt-Winters} model, specifically through the decomposition filter and try to identify seasonal trends (if they actually exist). Verify if the trends depend of the season/period of the year, some holidays or the geographical region. Such a model should offer new interesting insight (beyond daily and weekly patterns) into your data as well as reliable prediction results.
\end{enumerate}
This being said, it's safe to say that even if new models, beyond the ones researched in this paper, are implemented, the general pipeline still stands valid and is flexible enough to be customized to the needs of BMW. 